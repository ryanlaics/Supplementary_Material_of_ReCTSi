Thanks for your time and comments.

$^\S$For detailed visualizations of CaA and experimental results, please refer to [this link](http://bit.ly/49ADwMX).
### Q1 (Confusion of the Same Notation t):
Thanks! We'll use different notations in Eq. (4) and (6) to express their timestamp indices in the final version.
### Q2 (Assumption of CaA):
In CaA, the self-attention mechanism can address the long-term dependancy issue, where the features from either neighbor or distant segments can be processed equally in CaA. However, the vanilla self-attention mechanism lacks considering data reliability (which is indicated by the completeness of data segment). ReCTSiâ€™s CaA leverages the self-attention mechanism to evaluate both nearby and distant data segments, **focusing on their informational completeness rather than just proximity**. This is crucial because, in cases where a neighboring segment is largely missing, its contribution to understanding the current data point is minimal, rendering it less reliable. Conversely, a distant segment with complete data can offer valuable insights, deserving a higher weight in the analysis$^\S$. Thus, our CaA allows for better tradeoff between the temporal position impact and data completeness.
### Q3 (Rationale of Using Grouped FFN):
In fact, we have explored various methods to reduce the complexity in the TPA's attention modules, including adopting advanced transformer variants during ReCTSi's design. Despite these efforts, we found that existing alternatives led to accuracy losses for the imputation task (e.g., ReCTSi-Informer variant adopt the efficient attention mechanism from Informer, presents about 50% FLOPs decrease, while leading to 36% accuracy loss$^\S$, which is not acceptable). Consequently, motivated by Group Convolution, we then formulated the Grouped FFN mechanism. This adoption balances efficiency with effectiveness, avoiding the accuracy compromises observed in previous iterations. 
