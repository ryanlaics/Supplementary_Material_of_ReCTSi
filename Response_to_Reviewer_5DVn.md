We sincerely thank you so much for the positive evaluation of our work's novelty and quality!

$\S$ For detailed visualizations of CaA and experimental results, please refer to [this link](http://bit.ly/49ADwMX).
### Q1 (Confusion of the Same Notation t):
Thank you for highlighting this issue. We will use different notations in Eq. (4) and (6) to express their timestamp indices in the final version to avoid confusion.
### Q2 (Assumption of CaA):
In CaA, the self-attention mechanism can address the long-term dependancy issue, where the features from neither neighbour or distant segments can be processed equally in CaA. However, the vanilla self-attention mechanism cannot consider the data reliability (which is indicated by the completeness of data segment). ReCTSiâ€™s CaA leverages the self-attention mechanism to evaluate both nearby and distant data segments, focusing on their informational completeness rather than just proximity. This is crucial because, in cases where a neighboring segment is largely missing, its contribution to understanding the current data point is minimal, rendering it less reliable. Conversely, a distant segment with complete data can offer valuable insights, deserving a higher weight in the analysis $\S$. Thus, the consideration of data reliability and completeness allows for more accurate pattern extraction in ReCTSi.
### Q3 (Rationale of Using Grouped FFN):
In fact, we have explored various methods to reduce the complexity in the TPA's attention modules, including adopting advanced transformer variants during ReCTSi's design. Despite these efforts, we found that existing alternatives led to accuracy losses for the imputation task $\S$ (e.g, ReCTSi-Informer variant adopt the efficient attention mechanism from Informer, presents about 50% FLOPs decrease, while leading to 36% accuracy loss, which is not acceptable). Consequently, motivated by Group Convolution, we then formulated the Grouped FFN mechanism. The adoption of GFN balances efficiency with effectiveness, avoiding the accuracy compromises observed in previous iterations. 
